{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parliamentary Speaker's Rulings Classification Project\n",
        "\n",
        "This project aims to develop an automated text classification system to categorize Speaker's Rulings from the National Assembly of Zambia into predefined categories, enhancing accessibility for legal, parliamentary, and public use.\n",
        "\n",
        "\n",
        "# **1. Business Understanding**\n",
        "---\n",
        "\n",
        "\n",
        "## Problem Statement\n",
        "The National Assembly of Zambia generates substantial volumes of parliamentary proceedings containing Speaker's Rulings on various procedural, disciplinary, and administrative matters. Currently, accessing and categorizing these rulings for legal research, parliamentary reference, and civic education requires extensive manual effort. Legal professionals, researchers, parliamentary staff, and citizens face significant challenges in efficiently locating relevant precedents and understanding patterns in parliamentary decision-making.\n",
        "### Core Problem:\n",
        "There is no automated system to classify and categorize Speaker's Rulings from Zambian parliamentary proceedings, making legal and legislative information retrieval inefficient and limiting effective civic education and parliamentary research.\n",
        "\n",
        "## 1. Business Objectives\n",
        "\n",
        "### Primary Objectives:\n",
        "1. To classify parliamentary rulings into a structured system that is simpler to analyze and\n",
        "evaluate  \n",
        "2. Organize speakers' rulings to improve accessibility and quick retrieval for legal\n",
        "professionals, researchers, and public understanding.\n",
        "\n",
        "### Success Criteria from Business Perspective:\n",
        "- Reduce time spent by legal professionals searching for relevant rulings by at least 60%\n",
        "- Enable non-experts to find and understand parliamentary rulings relevant to their interests\n",
        "- Provide parliamentary staff with consistent categorization for improved procedural reference\n",
        "- Support academic and policy research through structured access to historical ruling patterns\n",
        "\n",
        "---\n",
        "## 2. Data Mining Goals\n",
        "### Specific Technical Objectives\n",
        "1. **Build a Multi-class Classification Model:**  \n",
        "   Develop a machine learning system that automatically categorizes Speaker's Rulings into predefined categories such as:\n",
        "    - **Procedural Rulings** (e.g., points of order, procedural motions)\n",
        "    - **Disciplinary Actions** (e.g., member conduct, sanctions)\n",
        "    - **Administrative Decisions** (e.g., scheduling, resource allocation)\n",
        "    - **Constitutional Interpretations** (e.g., constitutional questions, legal precedents)\n",
        "    - **Debate Management** (e.g., time allocation, speaking order)\n",
        "\n",
        "2. **Implement Natural Language Processing (NLP):**  \n",
        "   Apply text mining techniques to extract meaningful features from parliamentary text documents, enabling accurate classification.\n",
        "\n",
        "3. Create automated pipeline for new ruling classification.\n",
        "\n",
        "**Technical Success Metrics**\n",
        "* Accuracy: Achieve at least 80% overall classification accuracy across all ruling categories\n",
        "* Precision: Maintain precision above 75% for each individual category to ensure reliable\n",
        "categorization\n",
        "* F1-Score: Target F1-score above 77% for balanced precision-recall performance\n",
        "---\n",
        "## 3. Project Success Criteria\n",
        "### Quantitative Criteria\n",
        "1. **Model Performance:** Overall classification accuracy ≥ 80%\n",
        "2. **Processing Speed:** System should classify new documents within 5 seconds\n",
        "\n",
        "### Business Impact Measures:\n",
        "- Reduction in manual categorization time\n",
        "- Increased usage of parliamentary information by researchers and citizens\n",
        "- Improved consistency in referencing past rulings by parliamentary staff\n",
        "- Enhanced accessibility of legal and legislative information for civic education\n",
        "\n",
        "---\n",
        "## 4. Project Scope\n",
        "### in Scope:\n",
        "- Speaker's Rulings from National Assembly of Zambia proceedings\n",
        "- Text-based classification using parliamentary Hansards and official records\n",
        "- English language processing\n",
        "### Out of Scope:\n",
        "- Rulings from other parliamentary bodies or courts\n",
        "- Non-English parliamentary proceedings\n",
        "- Real-time audio/video processing of parliamentary sessions\n",
        "- Legal advice or interpretation of ruling implications\n",
        "### Constraints:\n",
        "- Data availability limited to publicly accessible parliamentary records\n",
        "- Solution must be cost-effective for potential implementation by parliamentary services\n",
        "- Timeline constraints require deliverable completion by August 29, 2025\n",
        "\n"
      ],
      "metadata": {
        "id": "aQWoB_1o1qJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P89pCxaFabvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Data Understanding**\n",
        "\n",
        "This notebook performs the **Data Understanding** phase for our data mining project using Google Colab.\n",
        "\n",
        "We work with the **Speaker’s Rulings** dataset and perfom Explorative data analysis (EDA) on the data to gain an understanding on the data\n",
        "\n",
        "## **Goals**\n",
        "- Explore structure (shape, columns, types) and preview records.\n",
        "- Profile data types, basic statistics, and distributions.\n",
        "- Assess data quality (missing values, duplicates, inconsistencies).\n",
        "- Summarize early insights and risks that will guide later preparation and modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "AbAXldJCKK3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Mount Drive for persistence of the Dataset**\n",
        "This allows to access the files directly and save work permanently.\n",
        "\n",
        "Run the code cell below. It will prompt you to authorize access to your Google Drive. Follow the on-screen instructions."
      ],
      "metadata": {
        "id": "-S2JE5mRKSJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Cm8-VRlKUSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once mounted, you can see your entire Google Drive by navigating through the \"Files\" pane on the left. It will appear under the drive/MyDrive/ directory.\n",
        "\n",
        "\n",
        "### **IMPORTANT NOTE**\n",
        "For this project, you should create a folder in your Google Drive (e.g., `rullings_classifier_data`) and upload the  datasets into it:\n",
        ">* `speaker_ruling_classification.csv`\n",
        "\n",
        "\n",
        " In this examples below, the file path used points to my own folder structure, for instance:\n",
        " >* `/content/drive/MyDrive/rulings_classifier_data`\n",
        "\n",
        " **You MUST replace this path with the correct path to the files in YOUR own Google Drive.** You can find the correct path by navigating to the file in the \"Files\" pane, right-clicking it, and selecting \"Copy path\"."
      ],
      "metadata": {
        "id": "7W38l9JJKTr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Environtment Setup and Data Loading**\n",
        "\n",
        "1. Importing the required python libraries\n",
        "  - **pandas** for handling and exploring data\n",
        "  - **pyplot** from matplotlib for plotting\n",
        "2. Load the Speaker's Rulling dataset into a pandas DataFrame using `pd.read_csv()`"
      ],
      "metadata": {
        "id": "U7oRTpGPKcAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Load dataset into a DataFrame\n",
        "file_path = \"/content/drive/MyDrive/rulings_classifier_data/speaker_ruling_classification.csv\"\n",
        "ruling_df = pd.read_csv(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8fQDD0yKZAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Basic Structure Analysis**   \n",
        "\n",
        "In this step, we examine the **basic structure** of the dataset to understand its size and scope.\n",
        "\n",
        "- **Dataset shape**: Number of rows and columns\n",
        "- **Total data points**: Rows * Columns.  \n",
        "\n",
        "This helps us estimate data volume and complexity before diving deeper.\n"
      ],
      "metadata": {
        "id": "naIwdV7MM9Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Datashape\n",
        "print(\"Dataset shape:\", ruling_df.shape)\n",
        "\n",
        "#Number of row (rulings)\n",
        "print(\"Number of rows(rulings):\", ruling_df.shape[0])\n",
        "\n",
        "#Number of columns (features)\n",
        "print(\"Number of columns(features):\", ruling_df.shape[1])\n",
        "\n",
        "#Total data points\n",
        "print(\"Total data points:\", ruling_df.shape[0] * ruling_df.shape[1])\n"
      ],
      "metadata": {
        "id": "-Pz-o-XDNeGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Column Structure Examination**\n",
        "\n",
        "To better understand the dataset, we analyze its **columns and data types**:\n",
        "\n",
        "- **`df.info()` summary** showing:  \n",
        "   - Column names  \n",
        "   - Number of non-null entries per column  \n",
        "   - Data types of each column  \n",
        "   - Memory usage  \n",
        "\n"
      ],
      "metadata": {
        "id": "vXfAC1JqOKqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#showing list of columns\n",
        "print(\"List of columns:\")\n",
        "for i , col in enumerate(ruling_df.columns,start=1):\n",
        "  print(f\"{i}.{col}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# showing a concise data summary including datatypes\n",
        "print(\"Summary of dataset:\")\n",
        "ruling_df.info()"
      ],
      "metadata": {
        "id": "lJZv92-5OM2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Initial Data Preview**\n",
        "\n",
        "A preview the dataset to gain a general sense of its content:\n",
        "\n",
        "- Use **`df.head()`** to view the first records.  \n",
        "- Use **`df.tail()`** to view the last records."
      ],
      "metadata": {
        "id": "YK_5NSY8Ota2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first 5 fow of the datase\n",
        "ruling_df.head()\n"
      ],
      "metadata": {
        "id": "sKqfp7qRPL2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#last 5 rows of the dataset\n",
        "ruling_df.tail(5)"
      ],
      "metadata": {
        "id": "-2KdN_WAPRRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Categorical Data Analysis**\n",
        "In this section we dive deeper in the Categorical columns *categories and context* columns to understand the distribution  within each column\n",
        "\n",
        "####What We'll Analyze:\n",
        "\n",
        "> - Value counts for each categorical column\n",
        "> - Number of unique values per column\n",
        "> - Most and least common categories"
      ],
      "metadata": {
        "id": "O2wmIqhcafIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of occurence for each category\n",
        "\n",
        "print(\"Number of occurence for each category:\")\n",
        "\n",
        "categoriesCount = ruling_df['categories']\n",
        "print(categoriesCount.value_counts())\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "Ad-zNWZ9aihv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Counting number of unique values in categories columns\n",
        "\n",
        "unique_category_values = ruling_df['categories'].unique()\n",
        "print(\"unique values in categories columns:\")\n",
        "print(unique_category_values)\n",
        "print(\"\\n\")\n",
        "print(\"Number of unique values in categories columns:\")\n",
        "print(len(unique_category_values))"
      ],
      "metadata": {
        "id": "i7Nsn73Tay9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of occurence for each context\n",
        "\n",
        "print(\"Number of occurence for each context:\")\n",
        "\n",
        "contextsCount = ruling_df['context']\n",
        "print(contextsCount.value_counts())\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "S77U1BIoa3J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#counting number of occurencies for each context\n",
        "\n",
        "unique_context_values = ruling_df[\"context\"].unique()\n",
        "print(\"unique values in context columns:\")\n",
        "print(unique_context_values)\n",
        "print(\"\\n\")\n",
        "print(\"Number of unique values in context columns:\")\n",
        "print(len(unique_context_values))"
      ],
      "metadata": {
        "id": "TT3KKR7na8M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Text Length and Word Count**\n",
        "This section focuses on fining:\n",
        "- The text length\n",
        "- Word count\n",
        "- Avarage word count for all the columns with paragraph\n",
        "including **rullingText** and **rullingTitle**\n",
        "\n"
      ],
      "metadata": {
        "id": "SeA1HzY5SGh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##creating a new column to hold the text length and column count for rulingTitle\n",
        "\n",
        "ruling_df['ruling_title_text_length'] = ruling_df['rulingTitle'].str.len()\n",
        "ruling_df['ruling_title_word_count'] = ruling_df['rulingTitle'].str.split().str.len()\n",
        "\n",
        "display(ruling_df.head(2))\n",
        "\n",
        "#finding the avarage word count and text length for the ruling title\n",
        "\n",
        "avarage_word_count = ruling_df['ruling_title_word_count'].mean()\n",
        "avarage_text_length = ruling_df['ruling_title_text_length'].mean()\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Avarage word count: \", avarage_word_count.astype(int))\n",
        "print(\"Avarage text length: \", avarage_text_length.astype(int))"
      ],
      "metadata": {
        "id": "36ior726SIgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Visual Analysis**\n",
        "Using pyplot we will Create categorical visualizations\n",
        "- Bar charts for top categorical variables\n",
        "- Pie charts for proportional data\n",
        "- Distribution plots for key categories"
      ],
      "metadata": {
        "id": "xvDC0MEySbQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating bar chart for categories column\n",
        "\n",
        "categoriesCount = ruling_df['categories'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Categories\")\n",
        "plt.xlabel(\"Categories\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.barh(categoriesCount.index, categoriesCount.values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pL6gQN_gScsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating bar chart for context column\n",
        "\n",
        "contextCount = ruling_df['context'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Contexts\")\n",
        "plt.xlabel(\"Contexts\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.barh(contextCount.index, contextCount.values)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0m--KqFdSvlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Feature relevance assessment**\n",
        "Based on the understand and research made upon the dataset\n",
        "\n",
        "we have identified that\n",
        "\n",
        "**Most important features:**\n",
        ">*  **rulingText:** This is the main body of the ruling, containing the richest semantic and contextual information.\n",
        ">* ***context*** is very noisy it might contribute little.\n",
        ">* ***categories*** This is meant to be the target column\n",
        "\n",
        "**Less relevant / redundant features**\n",
        ">* ***rulingTitle*** might be redundatant because it may be repetitive or just rephrasing the ***rulingText*** but nonetheless it can prove usefull when ***rulingText*** is not usefull\n",
        ">* ***standingOrder:*** this is potentially usefull but sparse, it contains values like 179, 53, 4 ...\n",
        "\n"
      ],
      "metadata": {
        "id": "_I2tVSbyTcAN"
      }
    }
  ]
}